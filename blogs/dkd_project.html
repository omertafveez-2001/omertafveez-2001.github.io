<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decoupled Gradient Knowledge Distillation</title>
    <!-- Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Bricolage Grotesque Font -->
    <link href="https://fonts.googleapis.com/css2?family=Bricolage+Grotesque:wght@300;400;500;600;700&display=swap"
        rel="stylesheet">
    <!-- Inter Font -->
    <link href="https://fonts.googleapis.com/css2?family=Hanken+Grotesk:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
    <!-- Lucide Icons (for professional icons) -->
    <script src="https://unpkg.com/lucide@latest"></script>
    <!-- CSS file outside this folder -->
    <link rel="stylesheet" href="../style.css">
</head>

<body class="subpixel-antialiased leading-none tracking-tight relative m-0 p-0">
    <!-- Header and Navigation -->
    <!-- Floating Navigation Menu -->
    <nav class="floating-nav">
        <a href="index.html" class="nav-item active">
            <i data-lucide="user" class="w-6 h-6"></i>
            <span class="tooltip">Home</span>
        </a>
        <a href="projects.html" class="nav-item">
            <i data-lucide="folder-open" class="w-6 h-6"></i>
            <span class="tooltip">Projects</span>
        </a>
        <a href="blogs.html" class="nav-item">
            <i data-lucide="book-open" class="w-6 h-6"></i>
            <span class="tooltip">Blogs</span>
        </a>
    </nav>

    <div class="scroll-watcher-bar"></div>
    <section id="hero" class="relative text-gray-900 py-2 md:py-8 flex items-center justify-center">
        <div class="absolute top-6 left-6 z-20">
            <a href="#" class="flex items-center gap-2">
                <svg width="100" height="40" viewBox="0 0 100 40" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <defs>
                        <linearGradient id="professionalGradient" x1="0%" y1="0%" x2="100%" y2="0%">
                            <stop offset="0%" stop-color="#1A202C" />  <!-- Dark Gray/Almost Black -->
                            <stop offset="100%" stop-color="#4A5568" />  <!-- Medium Gray -->
                        </linearGradient>
                        <linearGradient id="accentGradient" x1="0%" y1="0%" x2="100%" y2="0%">
                            <stop offset="0%" stop-color="#667EEA" />  <!-- Indigo -->
                            <stop offset="100%" stop-color="#805AD5" />  <!-- Violet -->
                        </linearGradient>
                    </defs>

                    <!-- 'O' shape - bold and geometric -->
                    <path
                        d="M20 5 L30 5 C35.5228 5 40 9.47715 40 15 V25 C40 30.5228 35.5228 35 30 35 L20 35 C14.4772 35 10 30.5228 10 25 V15 C10 9.47715 14.4772 5 20 5 Z M20 10 L20 30 L30 30 C32.7614 30 35 27.7614 35 25 V15 C35 12.2386 32.7614 10 30 10 L20 10 Z"
                        fill="url(#professionalGradient)" />

                    <!-- 'T' shape - integrated and modern, now overlapping the 'O' -->
                    <!-- Horizontal bar of T, starting earlier to overlap -->
                    <rect x="35" y="5" width="30" height="5" rx="1" fill="url(#professionalGradient)" />
                    <!-- Vertical bar of T, starting earlier to overlap -->
                    <rect x="47.5" y="5" width="5" height="30" rx="1" fill="url(#professionalGradient)" />

                    <!-- Removed the connecting line as it's no longer needed with the overlap -->
                </svg>
            </a>
        </div>
    </section>

    <section class="max-w-4xl mx-auto px-6 py-12 font-['Hanken_Grotesk'] text-gray-800">
        <h3 class="text-3xl font-semibold text-black-600">Bringing Balance Back into Logit Distillation: Decoupled Gradient Knowledge Distillation</h3>
    </section>
    <div class="justify-center max-w-4xl mx-auto px-6 pb-6 font-['Hanken_Grotesk']">
        <p class="text-gray-800 text-base mb-6">
            In recent times, knowledge distillation has become a go-to technique for transferring the ‚Äúwisdom‚Äù of a large, well-trained teacher model into a smaller, more efficient student model. The question we ask: 
            how best to transfer not just ‚Äúwhat‚Äù the teacher predicts but also ‚Äúhow‚Äù it reasons? <br>
            In this blog post we present our work on <strong class="text-blue-800">Decoupled Gradient Knowledge Distillation (DGKD)</strong> ‚Äî an implementation inspired by the paper <a href="https://arxiv.org/abs/2203.08679" class="text-blue-500">Decoupled Knowledge Distillation (Zhao et al., 2022)</a> ‚Äî and share both the intuition, our modifications, and takeaways.
        </p>
        <h5 class="font-semibold text-black-600 text-2xl mb-6">
            üß† Background: Why "Decoupled" Knowledge Distillation
        </h5>
        <p class="text-gray-800 text-base mb-3">
            The original DKD paper shows that the standard KD loss (e.g., the KL divergence between teacher and student logits) can be broken down into two parts:
        </p>
        <ul>
            <li class="mb-2">
                <strong> ‚Ä¢ TCKD (Target-Class Knowledge Distillation):</strong> the part of the distillation that focuses on the predicted target class ‚Äî essentially how confident the teacher is on the ‚Äúcorrect‚Äù class.
            </li>
            <li class="mb-4">
                <strong> ‚Ä¢ NCKD (Non-Target-Class Knowledge Distillation):</strong> the part of the distribution covering all other classes (i.e., the ‚Äúdark knowledge‚Äù of how the teacher distributes probability mass among wrong classes)
            </li>
        </ul>
        <p class="text-gray-800 text-base">
            The key insights from the paper are:
        </p>
    </div>

    <script>
        // Initialize Lucide icons
        lucide.createIcons();
    </script>