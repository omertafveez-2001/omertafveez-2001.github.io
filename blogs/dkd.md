---
layout: post
title: "Decoupled Distillation: Why Orthogonal Gradients Matter"
date: 2025-10-26
permalink: /blogs/dkd/
---

Exploring the efficacy of Decoupled Gradient Knowledge Distillation (DGKD) in encouraging functional gradient independence between target and non-target classes, leading to superior OOD generalization and feature compaction in smaller models.
